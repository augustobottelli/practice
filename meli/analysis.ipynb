{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Desafio Tecnico: Mercado Libre**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consignas**:\n",
    "- 1) Realizar un analisis exploratorio de las publicaciones con descuento del marketplace\n",
    "- 2) Armar un dataset y un modelo que permita predecir con atributos de la publicacion el valor de `sold quantity`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aclaracion:**\n",
    "En un primer momento opte armar un modelo general que permita predecir cualquier tipo de publicacion. El problema con este enfoque era que involucraba una complejidad de infraestructura que dada la escasez de tiempo, me enfrente pero decidi abandonar. \n",
    "\n",
    "Es por esta razon que opte por limitar el enfoque a predecir 5 tipo de productos cuyas caracteristicas estaban bien definidas y tenian muchas publicaciones: `computadoras`, `monitores`, `parlantes`, `notebooks` y `celulares`. Television fue una opcion pero el endpoint de search devolvia mas resultados de TV_STANDS que de TV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = get_token(APP_INFO[\"app_id\"], APP_INFO[\"secret_key\"], AUTH[\"refresh_token\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "productos = [\"notebook\", \"celular\", \"monitor\", \"parlante\", \"auriculares\"]\n",
    "final_df = get_dataset(productos, token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **EDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos las columnas con los descuentos\n",
    "final_df['has_discount'] = final_df['original_price'].notnull()\n",
    "final_df['discount'] = ((final_df['original_price'] - final_df['price'])/ final_df['original_price']).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observemos la distribucion general de descuentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(final_df.query('discount != 0')['discount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que el grueso esta entre 15% y 35%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, veamos la proporcion de productos que tienen descuentos agrupado por producto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props = final_df.groupby([\"domain_id\"])['has_discount'].value_counts(normalize=True)\n",
    "props_df = props.to_frame()\n",
    "props_df = props_df.rename(columns={'has_discount': 'prop'}).reset_index().sort_values('prop', ascending=False)\n",
    "filter_props = props_df.query(\"has_discount == True\")\n",
    "sns.barplot(x=\"domain_id\", y=\"prop\", color=\"salmon\", data=filter_props).set_title(\"Proporcion de publicaciones con descuento\")\n",
    "plt.xticks(rotation=60);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No pareceria haber muchas publicaciones con descuento, lo maximo es 5% para parlantes y ~4% para auriculares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos de los productos con descuentos, cual es el descuento promedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discounts = final_df.query('discount != 0')\n",
    "discounts = discounts.groupby('domain_id').mean().reset_index().sort_values('discount', ascending=False)\n",
    "g = sns.barplot(x=\"domain_id\", y=\"discount\", data=discounts, color=\"salmon\").set_title(\"Descuento promedio\")\n",
    "plt.xticks(rotation=60);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De todas maneras, el descuento promedio es mayor al 20% para todos los productos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que el grafico anterior era una estimacion puntual, tratemos de ver la distribucion de descuentos para cada producto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En histograma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.query('discount != 0')['discount'].hist(by=final_df['domain_id'], figsize=(15,15), bins=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En densidad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.set_style(\"darkgrid\")\n",
    "for main_category in final_df['domain_id'].unique():\n",
    "    temp = final_df.query('discount != 0')\n",
    "    temp = temp.loc[temp['domain_id'] == main_category]\n",
    "    sns.distplot(temp[['discount']], hist=False, label=main_category)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es llamativamente interesante como la mayoria de los descuentos para las notebboks se centra en 30% mientras que el resto se distribuye mas uniformemente entren el 10% y el 35%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamoslo en un boxplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.query('discount != 0').boxplot(column=['discount'], by='domain_id', figsize=(15,10), rot=60, color=dict(medians='r'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Wrangling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de procesar las columnas con formatos no convencionales, tratemos de limpiar un poco nuestro dataset. Se busca eliminar:\n",
    "- Las features que tienen valores unicos para cada observacion\n",
    "- Feature que tienen el mismo valor para todas las observaciones\n",
    "- Observaciones con outliers en nuestras variables numericas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por simplicidad, eliminemos las variales espaciales latitud y longitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.drop([\"seller_address.latitude\", \"seller_address.longitude\", \"geolocation.latitude\", \"geolocation.longitude\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que las variables price y base_price parecen ser iguales, veamoslo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(final_df['price'] != final_df['base_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.drop(\"base_price\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces, nuestras variables numericas son:\n",
    "- price\n",
    "- original_price\n",
    "- initial_quantity\n",
    "- available_quantity\n",
    "- sold_quantity\n",
    "- discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\"price\", \"discount\", \"available_quantity\", \"initial_quantity\", \"original_price\", \"sold_quantity\"]\n",
    "final_df[numeric_features].hist(figsize=(12,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[numeric_features].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bueno, tenemos un problema de outliers en casi todas las categorias... arranquemos por analizar los precios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.query(\"price > 1e05\")[['title', 'price']].sort_values('price', ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que estos precios son llamativos, sobretodo el precio de \"La camperita termica para perro\". Podriamos hacer un analisis para determinar cual deberia ser el precio que deberia corresponder a estos items, pero voy a optar por eliminarlos. Son las primeras 12 observaciones (hasta \"celular\") que parecen incorrectas, el resto parecerian estar relacionados al producto que venden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df[final_df['price'] < 9954695]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora observemos las cantidades vendidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[['title', 'sold_quantity', \"date_created\"]].sort_values('sold_quantity', ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parecerian ser todos numeros plausibles dada la antiguedad de las publicaciones, sigamos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora observemos las cantidades en stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[[\"title\", \"seller_id\", \"available_quantity\"]].sort_values(\"available_quantity\", ascending=False)[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pareceria ser que son casi todas las publicaciones de valores extremos pertenencen al mismo vendedor. Me llama la atencion que dispongan de semejante volumen aunque no parecen valores descabellados, creo. Ante la duda voy a optar por dejarlos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por las dudas, dada la distribucion asimetrica de las variables voy a crear variables que sean sus logaritmos, exceptuando a sold_quantity por ahora (aunque despues voy a fijarme si transformandola mejora la performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"price\", \"available_quantity\", \"initial_quantity\", \"original_price\"]:\n",
    "    final_df.loc[:, f\"log_{col}\"] = final_df[col].apply(lambda x: math.log(x+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora analizemos la variabilidad de nuestras features categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = list(final_df.drop(numeric_features, axis=1).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques ={}\n",
    "for col in categorical_features:\n",
    "    uniques[col] = final_df[col].apply(str).nunique() / len(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empecemos viendo las que serian unicas por publicacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(uniques.items(), key=lambda x: x[1], reverse=True)[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aca aparecen muchas de las variables unicas (id, permalink, descriptions, pictures, thumbnail, secure_thumbnail) que considero hay que eliminar, pero tambien una de las variables que en mi opinion, mas importa para predecir la cantidad de articulos vendidos, date_created. Veamos si start_time, historical_start_time y date_created son iguales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.drop([\"id\", \"permalink\", \"thumbnail\", \"secure_thumbnail\", \"descriptions\", \"pictures\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[final_df['start_time'] != final_df['date_created']][['start_time', \"date_created\", \"historical_start_time\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los casos en que no son iguales, que son pocos, tienen 1 segundo de diferencia, elimiemos start_time y historical_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.drop(['start_time', \"historical_start_time\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora transformemos un poco las columnas que tienen fechas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['days_old'] = (pd.Timestamp(datetime.utcnow(), tz=\"UTC\") - pd.to_datetime(final_df['date_created'])).dt.days\n",
    "final_df['days_remaining'] = pd.to_datetime(final_df['stop_time']).dt.days - pd.Timestamp(datetime.utcnow(), tz=\"UTC\")\n",
    "final_df['days_from_update'] = (pd.Timestamp(datetime.utcnow(), tz=\"UTC\") - pd.to_datetime(final_df['last_updated'])).dt.days\n",
    "\n",
    "final_df.drop([\"date_created\", \"stop_time\", \"last_updated\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora observemos si hay variables que tienen valores unicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(uniques.items(), key=lambda x: x[1])[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las variables cuyo valor es 2.79..e-05 tienen un solo valor. Eliminsemoslas dado que no aportan ningun tipo de informacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [key for key, value in sorted(uniques.items(), key=lambda x: x[1])[:17]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.drop(keys, axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, transformemos y adaptemos un poco las variables que tienen texto adentro como por ejemplo el titulo del la publicacion y la ciudad del vendedor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el titulo, quedemosnos pasemos todo a minusculas, eliminemos whitespaces y quedemosnos solo con las primeras dos palabras que usualmente tienen el nombre del producto y su marca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_title(x):\n",
    "    words = x.strip().lower().split(\" \")[:2]\n",
    "    parsed_words = \" \".join(words)\n",
    "    return parsed_words\n",
    "\n",
    "final_df.loc[:, 'title'] = final_df[\"title\"].apply(transform_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Me parece redundante tener 2 variables para decir lo mismo, asi que voy a optar por sacar o el id o el city name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[[col for col in final_df if col.startswith(\"seller_address\")]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[[col for col in final_df if col.startswith(\"seller_address\")]].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece ser que: \n",
    "- Para la ciudad, el nombre tiene mas informacion\n",
    "- Para la provincia, es indistinto\n",
    "- Para el barrio del search location el id es mas representativo\n",
    "- Para la ciudad del search location el id es mas representativo\n",
    "- Para la provincia del seach location, es indistinto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted_seller = [\"seller_address.city.id\", \"seller_address.state.name\", \"seller_address.search_location.neighborhood.name\", \n",
    "                   \"seller_address.search_location.city.name\", \"seller_address.search_location.state.name\"]\n",
    "final_df.drop(unwanted_seller, axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La unica coumna con nombre que nos quedo es para la ciudad del vendedor, pasemosla toda a minuscula y eliminemos whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.loc[:, 'seller_address.city.name'] = final_df['seller_address.city.name'].apply(lambda x: x.lower().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procesemos la informacion que esta adentro de tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"tags\"] = final_df[\"tags\"].apply(lambda x: \" \".join(x)).apply(lambda x: x.replace(\"-\", \"_\"))\n",
    "cvect = CountVectorizer()\n",
    "tags = cvect.fit_transform(final_df[\"tags\"]).toarray()\n",
    "for i, col in enumerate(cvect.get_feature_names()):\n",
    "    final_df[col] = tags[:, i]\n",
    "final_df.drop('tags', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtengamos los atributos espeificos de la publicacion. Para esto, voy a extraer del diccionario el ID del atributo como su valor. Elijo el value_name y no el value_id por azar, me parecia mas intuitvo para entender y ver la importancia de la feature dado que los puedo asociar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs_df = pd.json_normalize(final_df[\"attributes\"].apply(lambda lista: {d[\"id\"]: d[\"value_name\"] for d in lista}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([final_df, attrs_df], axis=1)\n",
    "dataset.drop(\"attributes\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un **dato de color** que me hizo **perder unas cuantas horas.....**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por algun motivo que desconzco, el join genero 12 observaciones con todo NA a pesar de que deberia haber hecho el join por indice como corresponde, eliminemoslas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[~dataset[\"sold_quantity\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(\"dataset.csv\", index=None)\n",
    "dataset = pd.read_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Estimador**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos nuestras variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset['sold_quantity']\n",
    "y_log = dataset['sold_quantity'].apply(lambda x: math.log(x+1))\n",
    "X = dataset.drop(['sold_quantity', \"date_created\", \"last_updated\", \"stop_time\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redefinimos nuestras features numericas y categoricas y transformamos el tipo para asegurarnos de no tener errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\"price\", \"log_price\", \"discount\", \"available_quantity\", \n",
    "                    \"log_available_quantity\", \"initial_quantity\", \"log_initial_quantity\", \n",
    "                    \"original_price\", \"log_original_price\", \n",
    "                    \"days_old\"]\n",
    "X[numeric_features] = X[numeric_features].applymap(np.float)\n",
    "categorical_features = list(X.drop(numeric_features, axis=1).columns)\n",
    "X[categorical_features] = X[categorical_features].applymap(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(\n",
    "    [('imputer', SimpleImputer(strategy='constant', missing_values=np.nan, fill_value=0))]\n",
    ")\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', missing_values=\"nan\", fill_value='__')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))]\n",
    ")\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(n_estimators=[200, 500, 800], \n",
    "              learning_rate=[0.1, 0.25, 0.4],\n",
    "              num_leaves=[30, 50, 80])\n",
    "estimator = GridSearchCV(\n",
    "        LGBMRegressor(n_jobs=-1),\n",
    "        params,\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        scoring=\"r2\",\n",
    "    )\n",
    "pipe = Pipeline([('preprocessor', preprocessor), ('estimator', estimator)])\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_feature_names = pipe.steps[0][1].transformers_[1][1].steps[1][1].get_feature_names(categorical_features)\n",
    "feature_importances = pipe.steps[1][1].feature_importances_\n",
    "\n",
    "cols = list(numeric_features) + list(ohe_feature_names)\n",
    "f_i = list(zip(cols, feature_importances))\n",
    "pd.DataFrame(f_i, columns=[\"feature_names\", \"importance\"]).sort_values('importance', ascending=False).reset_index(drop=True).query(\"importance > 0\")\n",
    "\n",
    "len(pipe.steps[1][1].feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=4)\n",
    "scores = cross_val_score(a, X_test[numeric_features], y_test, cv = cv, scoring='neg_mean_absolute_error')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
