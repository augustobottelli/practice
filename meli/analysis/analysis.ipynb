{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Desafio Tecnico: Mercado Libre**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consignas**:\n",
    "- 1) Realizar un analisis exploratorio de las publicaciones con descuento del marketplace\n",
    "- 2) Armar un dataset y un modelo que permita predecir con atributos de la publicacion el valor de `sold quantity`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aclaracion:**\n",
    "En un primer momento opte armar un modelo general que permita predecir cualquier tipo de publicacion. El problema con este enfoque era que involucraba una complejidad de infraestructura y extension de tiempo que, dada la escasez, enfrente pero decidi abandonar. \n",
    "\n",
    "Es por esta razon que decidi limitar el enfoque a predecir 5 tipo de productos cuyas caracteristicas estaban bien definidas y tenian muchas publicaciones: `computadoras`, `monitores`, `parlantes`, `notebooks` y `celulares`. Television fue una opcion pero el endpoint de search devolvia mas resultados de TV_STANDS que de TV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = get_token(APP_INFO[\"app_id\"], APP_INFO[\"secret_key\"], AUTH[\"refresh_token\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "productos = [\"notebook\", \"celular\", \"monitor\", \"parlante\", \"auriculares\"]\n",
    "final_df = get_dataset(productos, token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **EDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos las columnas con los descuentos\n",
    "final_df['has_discount'] = final_df['original_price'].notnull()\n",
    "final_df['discount'] = ((final_df['original_price'] - final_df['price'])/ final_df['original_price']).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observemos la distribucion general de descuentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(final_df.query('discount != 0')['discount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que el grueso esta entre 15% y 35%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, veamos la proporcion de productos que tienen descuentos agrupado por producto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props = final_df.groupby([\"domain_id\"])['has_discount'].value_counts(normalize=True)\n",
    "props_df = props.to_frame()\n",
    "props_df = props_df.rename(columns={'has_discount': 'prop'}).reset_index().sort_values('prop', ascending=False)\n",
    "filter_props = props_df.query(\"has_discount == True\")\n",
    "sns.barplot(x=\"domain_id\", y=\"prop\", color=\"salmon\", data=filter_props).set_title(\"Proporcion de publicaciones con descuento\")\n",
    "plt.xticks(rotation=60);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No pareceria haber muchas publicaciones con descuento, lo maximo es 5% para parlantes y ~4% para auriculares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos de los productos con descuentos, cual es el descuento promedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discounts = final_df.query('discount != 0')\n",
    "discounts = discounts.groupby('domain_id').mean().reset_index().sort_values('discount', ascending=False)\n",
    "g = sns.barplot(x=\"domain_id\", y=\"discount\", data=discounts, color=\"salmon\").set_title(\"Descuento promedio\")\n",
    "plt.xticks(rotation=60);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De todas maneras, el descuento promedio es mayor al 20% para todos los productos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que el grafico anterior era una estimacion puntual, tratemos de ver la distribucion de descuentos para cada producto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En histograma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.query('discount != 0')['discount'].hist(by=final_df['domain_id'], figsize=(15,15));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En densidad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.set_style(\"darkgrid\")\n",
    "for main_category in final_df['domain_id'].unique():\n",
    "    temp = final_df.query('discount != 0')\n",
    "    temp = temp.loc[temp['domain_id'] == main_category]\n",
    "    sns.distplot(temp[['discount']], hist=False, label=main_category)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es llamativamente interesante como la mayoria de los descuentos para las notebboks se centra en 30% mientras que el resto se distribuye mas uniformemente entren el 10% y el 35%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamoslo en un boxplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.query('discount != 0').boxplot(column=['discount'], by='domain_id', figsize=(15,10), rot=60, color=dict(medians='r'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Wrangling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de procesar las columnas con formatos no convencionales, tratemos de limpiar un poco nuestro dataset. Se busca eliminar:\n",
    "- Las features que tienen valores unicos para cada observacion\n",
    "- Feature que tienen el mismo valor para todas las observaciones\n",
    "- Observaciones con outliers en nuestras variables numericas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por simplicidad, eliminemos las variales espaciales latitud y longitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.drop([\"seller_address.latitude\", \"seller_address.longitude\", \"geolocation.latitude\", \"geolocation.longitude\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que las variables price y base_price parecen ser iguales, veamoslo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(final_df['price'] != final_df['base_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.drop(\"base_price\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces, nuestras variables numericas son:\n",
    "- price\n",
    "- original_price\n",
    "- initial_quantity\n",
    "- available_quantity\n",
    "- sold_quantity\n",
    "- discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\"price\", \"discount\", \"available_quantity\", \"initial_quantity\", \"original_price\", \"sold_quantity\"]\n",
    "final_df[numeric_features].hist(figsize=(12,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[numeric_features].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bueno, tenemos valorex extremos en casi todas las categorias... arranquemos por analizar los precios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.query(\"price > 1e05\")[['title', 'price']].sort_values('price', ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que estos precios son llamativos, sobretodo el precio de \"La camperita termica para perro\". Podriamos hacer un analisis para determinar cual deberia ser el precio que deberia corresponder a estos items, pero voy a optar por eliminarlos. Son las primeras 12 observaciones (hasta \"celular\") que parecen incorrectas, el resto parecerian estar relacionados al producto que venden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df[final_df['price'] < 9954695]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora observemos las cantidades vendidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[['title', 'sold_quantity', \"date_created\"]].sort_values('sold_quantity', ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parecerian ser todos numeros plausibles dada la antiguedad de las publicaciones, sigamos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora observemos las cantidades en stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[[\"title\", \"seller_id\", \"available_quantity\"]].sort_values(\"available_quantity\", ascending=False)[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pareceria ser que son casi todas las publicaciones de valores extremos pertenencen al mismo vendedor. Me llama la atencion que dispongan de semejante volumen aunque no parecen valores descabellados, creo. Ante la duda voy a optar por dejarlos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora analizemos la variabilidad de nuestras features categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = list(final_df.drop(numeric_features, axis=1).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques ={}\n",
    "for col in categorical_features:\n",
    "    uniques[col] = final_df[col].apply(str).nunique() / len(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empecemos viendo las que serian unicas por publicacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(uniques.items(), key=lambda x: x[1], reverse=True)[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aca aparecen muchas de las variables unicas (id, permalink, descriptions, pictures, thumbnail, secure_thumbnail) que considero hay que eliminar, pero tambien una de las variables que en mi opinion, mas importa para predecir la cantidad de articulos vendidos, date_created. Veamos si start_time, historical_start_time y date_created son iguales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.drop([\"id\", \"permalink\", \"thumbnail\", \"secure_thumbnail\", \"descriptions\", \"pictures\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[final_df['start_time'] != final_df['date_created']][['start_time', \"date_created\", \"historical_start_time\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los casos en que no son iguales, que son pocos, tienen 1 segundo de diferencia, elimiemos start_time y historical_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.drop(['start_time', \"historical_start_time\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora transformemos un poco las columnas que tienen fechas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['days_old'] = (pd.Timestamp(datetime.utcnow(), tz=\"UTC\") - pd.to_datetime(final_df['date_created'])).dt.days\n",
    "final_df['days_remaining'] = (pd.to_datetime(final_df['stop_time']) - pd.Timestamp(datetime.utcnow(), tz=\"UTC\")).dt.days\n",
    "final_df['days_from_update'] = (pd.Timestamp(datetime.utcnow(), tz=\"UTC\") - pd.to_datetime(final_df['last_updated'])).dt.days\n",
    "\n",
    "final_df.drop([\"date_created\", \"stop_time\", \"last_updated\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora observemos si hay variables que tienen valores unicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(uniques.items(), key=lambda x: x[1])[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las variables cuyo valor es 2.79..e-05 tienen un solo valor. Eliminsemoslas dado que no aportan ningun tipo de informacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [key for key, value in sorted(uniques.items(), key=lambda x: x[1])[:17]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.drop(keys, axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, transformemos y adaptemos un poco las variables que tienen texto adentro como por ejemplo el titulo del la publicacion y la ciudad del vendedor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el titulo, quedemosnos pasemos todo a minusculas, eliminemos whitespaces y quedemosnos solo con las primeras dos palabras que usualmente tienen el nombre del producto y su marca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_title(x):\n",
    "    words = x.strip().lower().split(\" \")[:2]\n",
    "    parsed_words = \" \".join(words)\n",
    "    return parsed_words\n",
    "\n",
    "final_df.loc[:, 'title'] = final_df[\"title\"].apply(transform_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Me parece redundante tener 2 variables para decir lo mismo, asi que voy a optar por sacar o el id o el city name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[[col for col in final_df if col.startswith(\"seller_address\")]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[[col for col in final_df if col.startswith(\"seller_address\")]].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece ser que: \n",
    "- Para la ciudad, el nombre tiene mas informacion\n",
    "- Para la provincia, es indistinto\n",
    "- Para el barrio del search location el id es mas representativo\n",
    "- Para la ciudad del search location el id es mas representativo\n",
    "- Para la provincia del seach location, es indistinto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted_seller = [\"seller_address.city.id\", \"seller_address.state.name\", \"seller_address.search_location.neighborhood.name\", \n",
    "                   \"seller_address.search_location.city.name\", \"seller_address.search_location.state.name\"]\n",
    "final_df.drop(unwanted_seller, axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La unica coumna con nombre que nos quedo es para la ciudad del vendedor, pasemosla toda a minuscula y eliminemos whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.loc[:, 'seller_address.city.name'] = final_df['seller_address.city.name'].apply(lambda x: x.lower().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procesemos la informacion que esta adentro de tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"tags\"] = final_df[\"tags\"].apply(lambda x: \" \".join(x)).apply(lambda x: x.replace(\"-\", \"_\"))\n",
    "cvect = CountVectorizer()\n",
    "tags = cvect.fit_transform(final_df[\"tags\"]).toarray()\n",
    "for i, col in enumerate(cvect.get_feature_names()):\n",
    "    final_df[col] = tags[:, i]\n",
    "final_df.drop('tags', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtengamos los atributos espeificos de la publicacion. Para esto, voy a extraer del diccionario el ID del atributo como su valor. Elijo el value_name y no el value_id por azar, me parecia mas intuitvo para entender y ver la importancia de la feature dado que los puedo asociar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs_df = pd.json_normalize(final_df[\"attributes\"].apply(lambda lista: {d[\"id\"]: d[\"value_name\"] for d in lista}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([final_df, attrs_df], axis=1)\n",
    "dataset.drop(\"attributes\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un **dato de color** que me hizo **perder unas cuantas horas.....**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por algun motivo que desconzco, el join genero 12 observaciones con todo NA a pesar de que deberia haber hecho el join por indice como corresponde, eliminemoslas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[~dataset[\"sold_quantity\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(\"dataset.csv\", index=None)\n",
    "#dataset = pd.read_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Estimador**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos nuestras variables, hagamos una pequeña prueba para ver si tomando logaritmos mejoramos el fit del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset['sold_quantity']\n",
    "y_log = np.log(dataset['sold_quantity'] + 1)\n",
    "X = dataset.drop('sold_quantity', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redefinimos nuestras features numericas y categoricas y transformamos el tipo para asegurarnos de no tener errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\"price\", \"discount\", \"available_quantity\", \"initial_quantity\", \n",
    "                    \"original_price\", \"days_old\", \"days_remaining\", \"days_from_update\"]\n",
    "X.loc[:, numeric_features] = X[numeric_features].fillna(0)\n",
    "X[numeric_features] = X[numeric_features].applymap(np.float)\n",
    "categorical_features = list(X.drop(numeric_features, axis=1).columns)\n",
    "X.loc[:, categorical_features] = X[categorical_features].fillna(\"__\")\n",
    "X[categorical_features] = X[categorical_features].applymap(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hagamos la prueba de la transformacion logaritimica usando nuestras variables numericas como predictores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X[numeric_features], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(n_estimators=[800, 1000], \n",
    "              learning_rate=[0.05, 0.1],\n",
    "              num_leaves=[20, 30])\n",
    "estimator = GridSearchCV(\n",
    "        LGBMRegressor(n_jobs=-1),\n",
    "        params,\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        scoring=\"r2\",\n",
    "    )\n",
    "model = estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5)\n",
    "scores = cross_val_score(model, X_test, y_test, cv = cv, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pareceria funcionar bastante bien, probemos tomando el logaritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X[numeric_features], y_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5)\n",
    "scores = cross_val_score(model, X_test, y_test, cv = cv, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay una mejora tomando el logaritmo. Ahora probemos un modelo con todas las features y veamos como fitea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(\n",
    "    [('imputer', SimpleImputer(strategy='constant', fill_value=0))]\n",
    ")\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='__')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))]\n",
    ")\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(n_estimators=[1000, 1500], \n",
    "              learning_rate=[0.1],\n",
    "              num_leaves=[30])\n",
    "estimator = GridSearchCV(\n",
    "        LGBMRegressor(n_jobs=-1),\n",
    "        params,\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        scoring=\"r2\",\n",
    "    )\n",
    "pipe = Pipeline([('preprocessor', preprocessor), ('estimator', estimator)])\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.steps[1][1].best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5)\n",
    "scores = cross_val_score(pipe, X_test, y_test, cv = cv, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No pareceria haber una mejora significativa por agregar la inmensa cantidad de features categoricos. De todas maneras, por todo el trabajo hecho, voy a optar por seguir el analisis de este modelo en general que, supongo, puede ser mas extensible a un predictor general de items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_feature_names = pipe.steps[0][1].transformers_[1][1].steps[1][1].get_feature_names(categorical_features)\n",
    "feature_importances = pipe.steps[1][1].best_estimator_.feature_importances_\n",
    "\n",
    "cols = list(numeric_features) + list(ohe_feature_names)\n",
    "f_i = list(zip(cols, feature_importances))\n",
    "feat_imp = pd.DataFrame(f_i, columns=[\"feature_names\", \"importance\"])\n",
    "feat_imp = feat_imp.sort_values('importance', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como esperabamos, las features numericas son las mas relevantes al momento de explicar la cantidad de ventas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = X_test.copy()\n",
    "preds = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformamos nuestros datos, recodemos que estan en logartimos, para analizar los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df['preds'] = np.round(np.exp(preds) - 1)\n",
    "preds_df['real'] = np.exp(y_test) - 1\n",
    "preds_df[\"error\"] = np.abs(preds_df['real'] - preds_df['preds'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fijemosnos como fitea nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.scatterplot(x=\"preds\", y=\"real\", data=preds_df)\n",
    "x = [x for x in range(ceil(preds_df['preds'].max()))]\n",
    "plt.plot(x, x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pareceria fittear bastante bien, veamos en valores de ventas mas chicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.scatterplot(x=\"preds\", y=\"real\", data=preds_df)\n",
    "x = [x for x in range(ceil(preds_df['preds'].max()))]\n",
    "plt.plot(x, x)\n",
    "plt.xlim(0, 1000)\n",
    "plt.ylim(0, 2000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que hay casos donde el error es significativo, hay un dato cuyo valor real es ~1100 y estamos prediciendo casi 10. Mas adelante nos vamos a concentrar en estos casos. Vayamos un paso mas, y veamos como fittea en valores aun mas chicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.scatterplot(x=\"preds\", y=\"real\", data=preds_df)\n",
    "x = [x for x in range(ceil(preds_df['preds'].max()))]\n",
    "plt.plot(x, x)\n",
    "plt.xlim(0, 200);\n",
    "plt.ylim(0, 500);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que hay errores interesantes a analizar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos si el error se asocia a algun tipo en proudcto en particular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df.groupby(\"domain_id\").mean().reset_index().plot.bar(x=\"domain_id\", y='error', title='Error promedio por domain_id', figsize=(12,8))\n",
    "plt.xticks(rotation=70);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que performa peor para auriculares y parlantes. Fijemosnos que paso con las observaciones que tenian errores muy grandes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = [\"title\", \"domain_id\", \"seller_id\", \"category_id\"]\n",
    "target_cols += feat_imp['feature_names'][:6].values.tolist() \n",
    "target_cols += [\"real\", \"error\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_df = preds_df.query(\"error > 200\")[target_cols].sort_values(\"error\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es dificil encontrar a ojo una relacion entre el error y nuestras features aunque se puede ver que el valor real correlaciona bastante bien con el error. Es de esperar ya que la mayoria de nuestras observaciones tienen pocas ventas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(errors_df.corr(), \n",
    "        xticklabels=errors_df.corr().columns,\n",
    "        yticklabels=errors_df.corr().columns,\n",
    "        center=0,\n",
    "        cmap=sns.diverging_palette(220, 20, as_cmap=True));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos que correlaciona positivamente con el total de ventas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podriamos seguir haciendo un analisis de inferencia causal con regresiones, significancia estadistica, intervalos de confianza, etc, para determinar las features que explican el error pero voy a optar por dejarlo aca ya que nuestro mayor problema es la asimetria en la distribucion de ventas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conseguimos un modelo bastante acertado para publicaciones de computadores, parlantes, celulares, monitores y notebooks. Un modelo con solo las variables numericas, para este contexto, parece ser la mejor opcion dada su liviandad de entrenamiento y carencia de preprocesamiento. De todas maneras, para predecir publicaciones en toda la categoria creo que las features categories pueden ser muy utiles dado que vemos (evaluando feature importance) que aportan informacion. Sortenado las dificultades de infraestructura, confio que el modelo puede funcionar para publiciaciones en general."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}